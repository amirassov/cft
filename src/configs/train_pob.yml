data_params:
    batch_size: 128
    num_workers: 0
    max_seq_len: 320

train_params:
    name: seq2seq_pob
    model: src.lm.models.Seq2Seq
    model_params:
        word_vec_size: 128
        rnn_type: LSTM
        hidden_size: 256
        num_layers: 2
        dropout: 0.2
        bidirectional: True
        use_attention: True
        share_embeddings: True
        tie_embeddings: True
    loss: src.lm.losses.BCELoss
    loss_params:
        size_average: True
        pad_id:
    steps_per_epoch: 50
    metrics: []

stages:
    -
        optimizer: Adam
        optimizer_params:
            lr: 0.001
            weight_decay: 0.00001
        scheduler: StepLR
        scheduler_params:
            step_size: 5
            gamma: 0.5
        epochs: 50
