data_params:
    batch_size: 128
    num_workers: 0
    max_seq_len: 320

train_params:
    name: seq2seq_test
    model: src.lm.models.Seq2Seq
    model_params:
        word_vec_size: 256
        rnn_type: LSTM
        hidden_size: 256
        num_layers: 3
        dropout: 0.3
        bidirectional: True
        use_attention: True
        share_embeddings: True
        tie_embeddings: True
    loss: src.lm.losses.BCELoss
    loss_params:
        size_average: True
        pad_id:
    steps_per_epoch: 50
    metrics: []

stages:
    -
        optimizer: Adam
        optimizer_params:
            lr: 0.001
            weight_decay: 0.00001
        scheduler: StepLR
        scheduler_params:
            step_size: 10
            gamma: 0.5
        epochs: 50
