data_params:
    batch_size: 256
    num_workers: 16
    max_seq_len: 100

train_params:
    name: teacher_forcing_seq2seq
    model: src.models.Seq2Seq
    model_params:
        word_vec_size: 128
        rnn_type: LSTM
        hidden_size: 256
        num_layers: 3
        dropout: 0.3
        bidirectional: True
        use_attention: True
        share_embeddings: True
        tie_embeddings: True
    loss: src.losses.BCELoss
    loss_params:
        reduction: elementwise_mean
        pad_id:
    steps_per_epoch: 1000
    metrics: []

stages:
    -
        optimizer: Adam
        optimizer_params:
            lr: 0.001
        scheduler: ReduceLROnPlateau
        scheduler_params:
            mode: min
            patience: 7
            factor: 0.5
        epochs: 200
    -
        optimizer: RMSprop
        optimizer_params:
            lr: 0.0001
        scheduler: CosineAnnealingLR
        scheduler_params:
            T_max: 15
            eta_min: 0.000001
        epochs: 200
